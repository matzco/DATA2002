---
title: "Untitled"
output: html_document
date: "2023-08-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Lecs/Tuts

## Test Statistic

Find test statistic. The difference between the data and the expected measure model.
This dataset is the percentage of people in categories business, gov, education, home using tablets in an old survey, as well as the count of people in a current survey. We are checking if the distributions match.

```{r}
y_i = c(102, 32, 12, 4)
p_i = c(0.69, 0.21, 0.07, 0.03)
n = sum(y_i)
e_i = n * p_i
t0 = sum((y_i-e_i)^2/e_i)
t0
```

## Chi Squared

In order to use a chi squared test, all the expected data points need to be >=5. 
To test for this, print out the e_i. 

```{r}
y_i = c(102, 32, 12, 4)
p_i = c(0.69, 0.21, 0.07, 0.03)
n = sum(y_i)
e_i = n * p_i
e_i
```

If there is a datapoint <5, reaggregate the datapoints to get an larger datapoint, at the cost of a category. Here, we will use the last 2 columns. As we are using the tablet use data, we can merge education and home.

Using the pchisq func, the second param is degrees of freedom. 
```{r}
y_i = c(102, 32, 12 + 4)
p_i = c(0.69, 0.21, 0.07 + 0.03)
n = sum(y_i)
e_i = n * p_i
t0 = sum((y_i-e_i)^2/e_i)
chis = pchisq(t0, 2)
```

This result gives the probability that the chi squared is greater/more extreme than the expected. /The probability that the datasets are completely unrelated. Therefore, we need 1-chi to find the p of them being related.

```{r}
1-chis
```
p is close to 1, so the data is close to the expected.


#Week 3

#L07

## Standard errors and confidence intervals for odds ratios
Definition: Odds Ratios - measure of the association between an exposure and an outcome.

Definition: prospective study - looking into future, based on current exposre/factors
           retrospective study - cohort of individuals with the outcome, looking at past factors
Odds ratio is usable in prospective and retrospective study
With odds ratios, what is the threshold to say an odds ratio is useful? eg 0.55
odds ratio of 1 indicates there is zero relationship
We will use a confidence interval. 
confidence interval for a sample mean: x +- Z * SE(x)
  z is critical value from normal or T distribution * standard error of x
    SE(x) = sample standard dev / sqrt(n)
  the z component the chosen z score
    z score is the standard deviation for that confidence percentage
      .99 is 2.58 standard deviations, .95 is 1.96 stddev, .90 is 1.645
      can be found in R with the percentage outside * 2
        qnorm(0.025)
  
Log odds scale
odds ratio is usually (0,infinity) with the neutral value being 1, but log odds ratio gives a more symmetric distribution centered at 0.
We can swap the x in the confidence interval formula for log(x) for the log odds ratio. 

Example OR = .55, log(.55) = -0.6
The SE = sqrt(1/104 + 1/189 + 1/10933 + 1/10845) = 0.12
x +- Z*SE(x) where Z 95% confidence interval
-> 
-.06 +- 1.96 * .12
= (-.84, -.36)
taking the exponent of these two decimals converts them back from log
(e^-.84,e^-.36)
= (.43, .69)

relating this back, we would have a null hypothesis that OR = 1. Our OR = .55, so we reject the null hypothesis, as it not in the range of the confidence interval. Same for the log odds, 0 is not in the log OR. 
This means that the odds of an individual having the outcome given the exposure is about .55, or flipped, an individual without the exposure is 50% more likely to have the outcome. This result is significant at 5% level of signifcance, because the confidence interval does not contain the null hypothesis.

##Testing for homogeniety

COVID treatments w plasma: 
          died discharged censored total
  plasma   : 5 28 6 39
  no plasma: 38 104 30 156
  total: 43 132 20 105

We want to answer was the proportion of deaths/survived different between the plasma treated patients vs no plasma?
Null hypothesis is there is no effect, the proportion is the same.

Two way contingency table
                  died survived total
plasma treatment: 5     28       33
no plasma treat : 38    104      142
                  43    132      175


```{r}
#This filters out all rows where col_name is "Undesired result"
#dataset = data %>% filter(column_name != "Undesired result")
  
```

##Dot notation subscripts for tables
When you see y1., take that as 'for all columns in y row 1'. Similarly z.2 'for all rows in column 2 in y'.

##Chi squared test of homogeniety
Using observed and expected counts of a table, we can make a chisq test statistic.
Pseudo: foreach cell: (observed - expected)^2 /expected
expected = row total * col total / overall total.
  totals = sum of individual row or col, overall is sum all rows.
Note this is a test with 1 degree of freedom, because (r-1)*(c-1). 

For the plasma/COVID table, the t0 = 1.9. 
The p-value = 0.16. Take a chisq with 1degree curve, and look at at the area under the curve from 1.9 and greater. In this curve, it is relatively significant, at .16. 
Given that the p-val is reasonably large, so there is a reasonable probability that the null hypothesis is correct, ie, plasma treatment has no solid impact. 

For a table bigger than 2x2, its the exact same, except the degrees of freedom are (r-1)*(c-1).

```{r}
#Voting preference example: H0: approve/not is the same across labor and lib voters.
y = c(62, 47, 29, 46, 9, 7)
n = sum(y)
tab = matrix(y, nrow = 2, ncol = 3)
colnames(tab) = c("Approve", "Not approve", "No comment")
rownames(tab) = c("Labor", "Lib")
tab
chisq.test(tab)
#pval = .045, under the .05 threshold, so we can reject H0.
```

#L08

##Testing for independence
Take a sample from a population, and ask 2 questions, and check if the answers are related.
(Contrast with homogeneity, where we take 2 sample populations and ask 1 question)
We use a contingency table, pretty much the same as in homogeneity. 
Mathematically can say P(x | y) = P(x). Other ways to write it too.
Again, use chisq.test(table, correct=FALSE)

##Degrees of freedom
Indicates the number of independent values that can vary in the sample. 
df = n of categories - 1 - n of parameters


#L09

##Fisher's Exact Test
Most tests require expected counts to be >5, so that we can guarantee the test statistic follows are chisq distribution. Sometimes we don't have alot of data, and the samples are all/partially less than 5.
This test determines if there is an association between two categories, where there is a 2x2 table, and or expected counts are <5. 
Attempting to use chisq.test() will generate a warning saying the cell counts aren't great enough.

fisher.test()

We are looking for the probability/p-value of getting a table with a top left value that is more extreme than the observed table. This will necessitate reducing the number in the top right, to keep the total population the same. So if the p-value is reasonably big, there's a good chance of getting a more extreme result, indicating that the observed result is within the bounds of H0. 

##Yates chisq
When we run chisq.test(), there's a parameter for correct="TRUE" or "FALSE". This changes the chisq test to include Yates' correction. Yates' correction introduces a -.5 to all differences between observed and expected cell counts. This is so that the approximation is marginally better with an extra 0.5. 


##Monte Carlo Test
For a contingency table where we know the observed values are >5, we can generate a bunch of randomised contingency tables with the row and col totals, so that all of the generated tables are comparable. 

```{r}
row_tot = 3
col_tot = 4.7
b = 100
set.seed(123)
#r2dtable: Random 2D table
table_list = r2dtable(n = b, r=row_tot, c = col_tot)

rnd.chisq = numeric(b)
for (i in 1:b) {
  rnd.chisq[i] = suppressWarnings(chisq.test(table_list[[i]])$statistic)
}
sum(rnd.chisq >= 6)/b

#can also use chisq.test with parameter (simulated.p.value = "TRUE")
```


#L10

##Testing Means

##One sample t-test
A t-test is used to determine if there is a significant difference between the means of two groups. 
These groups could be a sample and a population, two samples, etc. 

Demo example: actual mL of beer in a 6 pack, stated 375mL per.
```{r}
y = c(374.8, 375.0, 375.3, 374.8, 374.4, 374.9)
length(y)
t.test(y, mu = 375, alternative="less")
#p-value is large-ish = 0.15, so we don't reject H0.
```

##Two sample t-test
What if we are testing the population mean for two samples that are different?
The 2 could be independent, or related in some way, eg a before and after.


# L11

## Random Vars
Just random variables. The expectation of a set is the average value of the whole lot, and the expectation of sum and sum of expectation are equal. 
The variance is the spread of the values around the expectation (mean). The variance of the sum is not always the sum of variance.

Standard error of the means quantifies how much a sample mean will vary from one sample to another.

## Mean
Population mean is represented with mu, which looks kinda like a u.
Sample mean is represented with x^_ (x bar on top). 
Pop mean is the mean of the ENTIRE set, while sample mean is the mean of the sample.
If we want to know if u is a plausible value, we want to find the difference between the sample mean and the pop mean.
First, we find xbar, the sample mean. Then, find the standard error, and check the difference between SE and xbar - u.

## Standard Error
The measure of how much variation is likely in a sample mean compared to its population mean. 
SE = stddev / sqrt(sample size).\
```{r}
# sd(vector)
#std.error(statistic = mean, data = vector)
```
## Discrepancies
Given a fixed u/pop mean and observed sample mean x_, are we asking:
1. is x_ more than u? positive, 1 sided
2. is x_ less than u? negative, 1 sided
3. is x_ significantly different to u? both, 2 sided

We look for specific discrepancies in different cases:
Checking mL in beer cans - look for negative, where producers are underfilling, and also positive, where people drinking in excess of expectations
Weight of loaves: look for negative, scammy baker, but don't really care about customer winning a few grams

## Both Discrepancy
when values in the sample are on both sides of the population mean, we have a both discrepancy. 
Eg heights in school, u = 170cm, while x = [190, 160, 171, 172]: x_ = 173.It would appear that we have a positive discrepancy, but it is infact a both.

## False Alarm Rate
alpha is the desired false alarm rate, when we incorrectly reject a given u/pop mean. 0 <=a<=1, smaller is better, eg a = 0.05,.01.

Why allow false alarm rate >0 at all!?
The you would never reject anything then, even if you should. The test would have no power (p correctly reject H0)

## Quantiles
To get R to display charts and values, use
t-values: qt(p, df = k)
eg a = .05, we take p = 1 - a/2
normal: qnorm(p)
chisq: chisq(p, df = k)

## Confidence Interval
The coverage probability is the probability that the true value is within the confidence interval.
We want a small non coverage probability (just the inverse), like 1-a.

## p-value
the p-value is the smallest false alarm rate for which we would reject a given u, and the non-coverage (1-p) for which u is on the boundary of the confidence interval.

## Rejection region
We define a decision rule to reject H0, where if p < a, we reject H0. For a 2 sided test, there will be rejection regions on both ends of the distribution curve. 


# L12

## Types of Errors
Type I: false positive, reject a true H0
Type II: false negative, accept a false H0

Power in hypothesis testing: the probability of correctly rejecting the H0.

## Statistical power in one sample t-test
P(reject H0) = P( abs(x_ - u) / S/sqrt(n) > c)
Power is 0 <= P <= 1.
0: test has no ability to correctly reject H0
1: test will always correctly reject H0.
Power is almost never 1

## t-statistic
X_ - u / S/sqrt(n)

## Cohen's d
Quantifies the size of the difference between effect sizes between two populations.
d = abs(mean pop1 - mean pop2) / stddev(pop1&2)
will be [0,1]
values of .2, .5, .8 indicate small med large effect sizes.


# L13

## Paired t-test revision
Testing the similarity of the mean of two related populations. 
eg testing for a difference in mean between one leg w placebo and one with chem injection

## Normality
We always like to assume data is sampled from a normal population. For massive samples, we can rely on CLT to assume the test statistic follows a t distribution, but not for small samples.
We can check normality using QQ plots and boxplots.

## Central Limit Theorem Normality CLT
When taking a sample from a large population, regardless of shape of the original data, the random sample is approximately normal. 
CLT requires random sampling, independence in the sample, and finite variance of the original data.


## QQ plot
We want to see that the points all lie reasonably close to the line. If there are any/several massive outliers, the sample might not be normal.
```{r}
# Set a seed for reproducibility
set.seed(123)

# Generate a random sample of 100 observations from a normal distribution
random_data <- rnorm(100)

# Create a Q-Q plot to assess normality
qqnorm(random_data)
qqline(random_data, col = "red")  # Add a reference line

# Add a title and labels
title("Q-Q Plot for Normality")
xlabel <- "Theoretical Quantiles"
ylabel <- "Sample Quantiles"
xlabel <- "Theoretical Quantiles"
ylabel <- "Sample Quantiles"

```
## Sign test
Suppose we take a sample from a continuous distribution, we want to check the mean of our sample vs pop.
H0: u0 = u / usample = upop
If the population is symmetrical around u0, then each population value minus sample mean should land around 0. Because of the symmetry, the likelihood of Xi - u0 to be +/- is equally likely, 0.5. 

Sign test is non parametric because is makes fewer assumptions, it only looks at observed values, not means/medians. 

```{r}
#binom.test(c(6,3), p = .05, alternative="greater")
# c(positives, falses)
```
The p-value of a sign test and t-test can give conflicting results when sample size is small.

Sign test is not used as a normality test, but rather as a alternative t test. 

# L14 

## Wilcoxon signed rank test
Frank Wilcoxon is notably not a New Yorker nor Jewish, still American.
A sign test ignores alot of factors, such as low power. 

We use ranks, whereby we give each item in a sample a rank, from 1 being the lowest, up to n, being the size of the sample. If there are ties, each tied item is given the average of the other tied items ranks. 

Sample: 1 2 2 5 8 8 8 10
ranks : 1 2 3 4 5 6 7 8 -> 1 2.5 2.5 4 6 6 6 8

If we get all the items from the sign test, being the sample items - mean, and apply their ranks, we would expect them to be nearly equal for positive and negative sides.

We sum up all the ranks for the positives = W+, and sum up the ranks for negatives, being W-.
Take W = min(W-, W+).
If W+ is large, we can reject H0, as there is indication that u > u0, and if W+ small, indicates u < u0.

## p-value in Wilcoxon signed test
P(W+ >= w+) = P(W+ <= n(n+1)/2 -w+)
The probability of getting an observed W greater/equal than a W expected according to the null hypothesis is equal to the chance of getting a value smaller than a set critical value, which is based on sample size. 
n(n+1)/2 is the maximum possible value for W in the W sign test, where n is num of pairs. 
If n(n+1)/2 -w+ is small it indicates w+ is larger than would be expected for H0, and there is a significant difference between the observations.
```{r}
n = 5 # sample size
q = 0:(n * (n + 1)/2)# possible values for sum of the positive ranks
probs = dsignrank(q, n)
names(probs) = q
mu = n * (n+1)/4
s2 = n * (n+1) * (2*n+1)/24
df = data.frame(
  prob = dsignrank(q, n), 
  x = q)
df
```

## Normal approximation for Wilcoxon sign test
For a large enough sample, we can approximate the distribution of w. 
T = w+ - E(W+) / sqrt(var(W+))
where E(W+) = n(n+1)/4 and var(W+) = n(n+1)(2n+1)/24.

# L15

## Wilcoxon rank sum test
Non-parametric (doesn't make assumptions) to compare the means of two different samples. They don't have to be normally distributed, but they must have the same distribution, eg left skew. 
We rank all of the items together, and then sum the item ranks based on sample. If H0, we expect W to be close to the expected value

```{r}
#the pwilcox function distribution starts from 0, so we need to subtract minw.
#pwilcox(w - minw, m = nx, n = ny)
```
 
What the fuck is this

# L16

## Permutation Tests
Brit called Ronald Fisher testing a lady claiming she can taste whether milk was added before or after tea. Its kinda impractical to test this 100 times, so lets only make 8 cups, with 4 of each. 
In this instance the test statistic is the number of milk before teas correctly identified. 

The order of the 8 cups is random, and there are 8! =40320 possible orderings, but there are only 8C4 =70 ways to pick which 4 cups had milk added before. 

Assuming random guesses, the probability of getting 0 right is 4C0 * 4C4 = 1, T(1) = 4C1*4C3 = 16, T(2) = 36, T(3) = 16, T(4)=1.
Remember all of these are the number of ways/70, so P(T(4)) = 1/70 = 0.014.

## Generate permutations
Since the lady can only manage 8 cups of tea, we generate simulated permutations, 8! of them.
```{r}
library(arrangements)

actual_order = c("milk","tea","tea","milk","tea","tea","milk","milk")
predicted = c("milk", "tea", "tea", "milk", "tea", "tea", "milk", "milk")

#permutations() func generates all permutations
tea_permutations = permutations(actual_order)

#create an empty list which we'll use to store checked values for each permutation
check_correct = vector("numeric", length = nrow(tea_permutations))
for(i in 1:nrow(tea_permutations)) {
  #init each index of check_correct with 1 or 0 if the permutation matches the prediction
  check_correct[i] = identical(tea_permutations[i,], actual_order)
}
#576 and 0.014
c(sum(check_correct), mean(check_correct))

```
576/40320 permutation rows were correct, which gives P(4) = 0.014. 

now lets try again with fishers exact test, should give same result.
```{r}
truth = c("milk", "tea", "tea", "milk", "tea", "tea", "milk", "milk")
predicted = c("milk", "tea", "tea", "milk", "tea", "tea", "milk", "milk")
tea_mat = table(truth, predicted)
fisher.test(tea_mat, alternative = "greater")$p.value
```

## Permutation sampling
If there are too many permutations, we can use a random sample from them using sample(). This should give a decent approximation
```{r}
set.seed(123)
truth = c("milk","tea","tea","milk","tea","tea","milk","milk")
B = 10000
result = vector(length = B) # initialise outside the loop
for(i in 1:B){
  guess = sample(truth, size = 8, replace = FALSE) # does the permutation
  result[i] = identical(guess, truth)
}
mean(result)
```

## Permutation test for 2 independent samples
Do a t-test, get the t-test statistic with t.test$statistic

Do the permutations, and t_null will give the test statistics of the permuted samples. 
Take the absolute values of the t_null and test statistic
Check the difference between the means
mean(abs(t_null) >= abs(tt$statistic))
this shows the proportion of permutated test statistic that more extreme than the observed one

```{r}
dat = PlantGrowth |> filter(group %in% c("ctrl", "trt2"))
B = 10000 # number of permuted samples we will consider
permuted_dat = dat # make a copy of the data
t_null = vector("numeric", B) # initialise outside loop
for(i in 1:B) {
  permuted_dat$group = sample(dat$group) # this does the permutation. each sample is just a random order of dat
  t_null[i] = t.test(weight ~ group, data = permuted_dat)$statistic
}
```

## Permutation test for paired sample 

Testing for shift between paired population. Lets measure platelets in 11 people before and after a durry. 
```{r}
# Given data
before = c(25, 25, 27, 44, 30, 67, 53, 53, 52, 60, 28)
after =  c(27, 29, 37, 36, 46, 82, 57, 80, 61, 59, 43)  
d = after - before  # Calculate the differences

# Perform a one-sample t-test
t.test(d)

# Calculate the t-statistic for the original data
(t0_original = mean(d) / sd(d) * sqrt(length(d)))

n = length(d)  # Number of observations

# Generate all possible sign permutations for a two-sided test
sign_permute = permutations(c(-1, 1), 11, replace = TRUE)

# Get the dimensions of the sign_permute matrix
dim(sign_permute)

B = nrow(sign_permute)  # Number of permutations

t_null = vector("numeric", B)  # Initialize a vector to store permuted t-statistics

# Loop over each permutation and calculate permuted t-statistics
for(i in 1:nrow(sign_permute)){
  d_permute = d * sign_permute[i,]  # Apply the sign permutation to the differences
  t_null[i] = mean(d_permute) / sd(d_permute) * sqrt(n)  # Calculate the t-statistic for each permutation
}

# Calculate the proportion of permuted t-statistics as extreme as the original
mean(abs(t_null) >= abs(t0_original))

```
This code performs a two-sided permutation test for paired differences between the "before" and "after" measurements. 
It first calculates the observed t-statistic for the original data and then generates all possible sign permutations of the differences to create a distribution of permuted t-statistics. 
The final line calculates the p-value by determining the proportion of permuted t-statistics as extreme as the original and tests whether there is a significant difference between the "before" and "after" measurements

# L17

## - Speed of light
Canadian oke estimated the speed of light, timed it over 7km 66 times, in 1882. cool

## Estimation vs Hypothesis
Estimation: a population parameter is unknown. We use sample statistics to generate estimates for it.

Hypothesis: explicit statement about the population parameter. Test statistics are generated to support or reject the hypothesis.

## Confidence intervals
avoid reporting just an estimate for a sample, include a margin of error.
margin of error = critical value * SE(estimate)
critical value aka z-score
  [shift click for z-score explanation](## Standard errors and confidence intervals for odds ratios)

Point estimates don't convey information about the variability or range of an estimate.

## Confidence level

If the probability of a point estimate falling in an interval of test statistics is 1, it is a 100% confidence interval. 
P(t1 <= point estimate <= t2) = 1 - a
if a = 0, the confidence level is 100%, so we choose a to be 0.01 for 99%, .05 for 95%, etc.

This doesn't guarantee a point estimate will be in a range, just that 95% of the possible ones will be.
Confidence != probability

## Bootstrapping
A resampling method with replacement, useful when data does not follow a normal distribution.
also useful for small sample sizes
non parametric, don't need to make parametric assumptions
provides answers when no analytic solutions 
can be used for verification
asymptotically consistent

## Bootstrap confidence intervals
the 100% confidence interval of a bootstrapped point estimate is where the interval is between the a/2 and 1-a/2 quantiles in the distribution.

We can get the 95% interval using this, where result = the target metric 
quantile(result, c(0.025, 0.975))

# L18

Recap


# L19

## Multiple testing

That xkcd about the colour of jellybeans and autism, where if you do a t-test 20 times with a = 0.05, one of them is will hit.
A rejected null hypothesis can't be taken at face value. 

## Erorr rates formulae

False positive rate: null results are accidentally called significant
number of p-values < .05 when they aren't really / number of tests

Family wise error rate: probability of at least one false positive
P([a p-value > .05] >= 1)

False discovery rate
number of false positives / number of rejected hypotheses

## family wise error rate
The probability of at least one false positive

FWER P(V >= 1), V: number of false positives
the probability of at least one false positive is 1 - (1- a)^m, m - number of hypotheses

## Bonferroni

makes a new threshold for significance. 
a* = a / m
pretty high bar, but keeps FWER <= a. 
could be too high, but easy to use.
Usually sufficient

## False discovery rate
Aim: keep the expected proportion of false positives in the rejected tests close to a
FDR = E (num false positives / num false negatives)

## Benjamini-Hochberg Procedure
Most popular correction for performing lots of tests.
Calculate p-values normally
order all the values in ascending order
find max(j* ) where p( j* ) <=  a*j/m
Reject all H0 where p(H0) <= a(j*/m)

still pretty easy to calculate, slightly lower bar
allows for more false positives
doesn't work great with dependence

# L20

## ANOVA
ANalysis Of VAriance. 

Generalisation of a two sided two sample t test for 3 or more samples.

recap: 3 different procedures for 2 sample t test
paired, welch, classical/pooled

All use the form X - Y / SE(X - Y), but differ in how standard error is calculated.

Of the 3 methods, the classical requires the most assumptions. 
The Welch test is the default in R. The paired test can be used if sample sizes are equal
But classical is the one that generalises to ANOVA, but we must be aware of the assumptions of independence between samples, and equal variance

```{r}
data(mtcars)

# Perform a one-way ANOVA to test the impact of the number of cylinders on car mpg
# In this example, we're comparing the mean mpg (miles per gallon) among different cylinder groups (4, 6, and 8)
# Adjust the formula and dataset as needed for your specific analysis
anova_result <- aov(mpg ~ cyl, data = mtcars)

# Summarize the ANOVA results
summary(anova_result)

#p<.05, so means are very different

```


## ANOVA vs two sample t test

Purpose: 2 sample t-test compares the means of 2 independent groups, ANOVA is for 3+.

Hypotheses: 2 sample H0: the means are equal, H1: they're not, ANOVA: all group means are equal, H1: at least one group is different, others equal

Test Statistic: 2 sample t test: test statistic, ANOVA: F-statistic, compares group means to variance within groups

Multiple comparisons: 2 sample: when doing multiple t-tests, you need to account for FWER with bonferroni, ANOVA: considers overall already so not needed, but you can use Bonferronis to find which specific group differs.

## ANOVA specifics

H0: all groups have the same mean.
H1: at least 1 group has a different mean
Assumed: Samples are independent. Each sample has the same variance. Each sample is normally distributed (or is large enough for CLT)
Test statistic: t = treatment mean / residual mean
  treatment mean: variability between group means
  residual mean: variability within each group
Decision: if the p-value <= a, reject H0, as at least one group mean is different to the others. 

## Double subscript notation
i refers to index of the samples
j refers to index observation within samples
g is the number of groups

dot notation indicates adding for all, so yi. means sum of all observations j in yi.
bar over the y is for average, average for sample 4 is \overline{y}4. .
total of all obs is y..
average for all observations is \overline{y}..

## General ANOVA decomposition
partition the total variability in a datset into different sources of variation
treatment effect: inter-group mean comparison
residual effect: intra-group mean

## Sum of Squares Total SST
SST: total sum of squares, represents total variability in the data, without distinguishing between groups. Quantifies deviation of observations from overall mean. 

SST = SSB + SSW

## Inter-group variability/treatment effect
Measures variation among group means
SSB = for each group, sum {observations in group * (mean of obs in group - overall mean)^2}

## Intra-group variability/residual effect
SSW = for each group: sum of all (observations- overall mean observations)^2

# L21

## ANOVA contrasts
How do we compare 3 different means, H0 is they're all equal.

If p-value <= .05, reject H0, but this doesn't directly tell us which group mean is different. 

A contrast is a linear combination where all coefficients sum to 0, and in ANOVA, a contrast is the linear combination of means. The contrast is used to compare specific groups.
If H0 is correct, the contrasts will all be 0 too. 

## Calculating contrast
First, set the coefficients for each group. Set the target group 1, the other target -1, and other groups 0. 

Second, calculate the contrast value. This is the coefficient of A * uA + coeff B * uB ...
This should come out to c = uA - uB or similar, where uA = mean of A. 

Third, test if the contrast value is statistically significant, like using an F test or t test. 

## Confidence intervals of contrasts
ok moving on




# L22

## Using residuals to check normality
Rather than looking at QQ plots we can instead look at ANOVA residuals. If ANOVA assumptions are true the residuals should be normal.

## Multiple comparisons and simultaneous confidence intervals
When no single group is notable, we consider each pair equally interesting. 

## Bonferroni

## Multiple comparisons: pairwise t test

## Tukey method

## Scheffe method

## Concluding
ANOVA Ftest might not always be enough, contrasts might be more significant.
Bonferroni is pretty conservative, Tukeys is less so
Contrasts must be picked before checking out the data.
Scheffes method permits unlimitied snooping, checking contrasts afterwards


# L23

# L24

# L25


# Module 4: Learning and prediction


# L26

## Learning
Supervised = known classes or values, train to predict new
Unsupervised = unknown, determine groupings

Supervised can be broken to classification or regression
Classification maps input to output label, regression maps to continuous input

## Simple linear Regression
aims to predict an outcome Y based on single input predictor. Just a straight line
Y = population intercept + pop slope * input + error term

## Fitting the line
aim to minimise the sum of squared residuals

in R, use lm()
Fitted values are the ones obtained from using the observed values in the model. Residuals are the differences between the observed value and the model's estimated Y.

lm$resudials

## The lm() function for simple linear 
```{r}
# formula = the target var ~ the predicting var. data = the dataset
model = lm(formula = mpg ~ hp, data = mtcars)

```


## Checking assumptions
4 assumptions for a linear regression model:
1. linearity - the relationship between x and Y is linear
2. independence - all the errors are independent 
3. homoskedascticity - all the errors have constance variance
4. normality - all the errors have a normal distribution

## 1. linearity
before running a regression, plot x/Y and check if its approximately linear. then plot the residuals, they should be symmetric around 0. If there is a distinct pattern, the residuals are non linear. 

## 2. independence
independence between errors is usually dealt with before data collection. 

## 3. homeskedasticity
homo = same, skedasticity = spread
the data should not be clustered, there should be a fairly even spread of data points across the expected distribution.

## 4. normality
easiest to check a QQ plot of residuals


#L27

## Notation
B0 = intercept
B1 = input coefficient which is also the line slope
\hat{Y} = predicted value
Y = actual value
epsilon = residuals

## interpreting model coefficients
The intercept is the expected value Y when the input x = 0. 
example: Y (ozone) = -1.84 + 0.67 * temp, intercept = -1.84

## models with log transformations
log(Y) = B0 + B1* x : one unit increase in x will make B1* 100% in Y
Y = B0 + B! * log(x): 1% increase in x makes B1/100 in Y
log(Y) = B0 + B1* log(x): 1% increase x makes B1% in Y

## Inference in regression models
Normally for linear regression H0: B1 = 0, H1: B1 != 1
B1 is the coeff of input.

T = slope of line / SE(slope line) [n-2 degrees freedom]
We reject H0 if p < .05, BUT look at the data, check for other assumptions before relying on p-values.

## Confidence intervals
as usual.
B1 +- t* SE(B1) where t*: a/2 quantile from t distribution with n-2 DF.

Use confint(linear model name)


## In sample performance
Total sum of squares = 
SS for regression line: sum(hatY - mean Y)^2 +
SS for residuals: sum(y - expected y)^2

## Coefficient of determination r^2
The square of the correlation coefficient aka coefficient of determination measures the proportion of total variation explained by the linear regression model. 

"one minus the proportion of variation not explained by the model"
r^2 = 1 - resid SS / total SS

it is a % of variation in y as explained by x.


## Multiple regression
What if we ask if more factors can be used to predict? eg if temperature, wind, and radiation can predict log(ozone)?

Y = B0 + B1 * x2 + B2 * x2 + B3 * x3 ... + e

log(ozone) = B0 + B1* radiation + B2 * temp + B3 * wind + epsilon

Again, just use lm()
```{r}
multireg = lm(formula = mpg ~ cyl + hp + qsec, data = mtcars)
summary(multireg)$coefficients |> round(3)

# the estimates are the coeffs
# Y = 55 - 2.3*cylinders - 0.04*horsepower - 0.9*quartermile time

#this is probably a bad example, because var are like certainly dependant
summary(multireg)$r.squared
```
Often written in matrix formula:
Y = X*B + e
where Y, X, B are vectors.

## Interpretation of Multiple regression coefficients
The estimated coefficients are conditional on the other coefficients/inputs not varying. 
Meaning the -2.2 coefficient of cylinder count is conditional on horsepower and 1/4mile time NOT changing, so any calculation for delta cyclinder must assume that nothing else is being adjusted. 

example: increasing cylinder count by 1 decreases mpg by 2.2%, holding hp and qsec constant. 
increasing hp by 1 decreases mpg by .04%, holding cyl and qsec constant. 
```{r}
summary(multireg)$r.squared
```
The r^2 is .77, so 77% of the variance in Y is explained by the regression model.


# L28

## Prediction
Once we have a model, we can plug in inputs to find an expected value.

```{r}
fakef1car_obs = data.frame(cyl = 8, hp = 200, qsec = 25)
print("prediction interval for a new datapoint")
predict(object = multireg, newdata = fakef1car_obs, interval = "prediction", level = .9)

# can also use interval = "confidence"
print("confidence interval for a datapoint")
predict(object = multireg, newdata = fakef1car_obs, interval = "confidence", level = .9)
# fit      lwr      upr
# 1 7.6883 -0.23854 15.61514
# fit: the predicted value, 7.6 mpg
# lower and upper bounds
```
## prediction vs confidence interval
the prediction interval predicts an individual number, but confidence interval predicts a mean value. 
prediction is for a new individual datapoint, confidence is for a range of plausible values for certain conditions. 

prediction: y +- t* sqrt(var(residual y))
confidence: y +- t* sqrt(var(y))

## Categorical predictors
using the mpg dataset, for fuel economies, and the vehicle class factor.
```{r}
data("mpg", package = "ggplot2")
lm_mpg = lm(cty ~ class, data = mpg)
anova_mpg = aov(cty ~ class, data = mpg)
```

lm and ANOVA do the same thing/same results, same errors. 
lm is better for coefficients, linear data, and specific hypotheses.
ANOVA is for group differences

## marginal means and emmeans()
marginal means aka least squares means or adjusted means are for estimating means for each level of a categorical predictor, that also accounts for the effects of other factors in the model. 

```{r}
library(emmeans)
data("mpg", package = "ggplot2")
lm_mpg = lm(cty ~ class, data = mpg)
em_mpg = emmeans(lm_mpg, ~class)
em_mpg
```

emmeans() estimated marginal means. 
function to compare marginal means for different combinations of factors, especially for categorical predictors. the emmean() will output emmean which we can use to easily compare the different categories


# L29

## Model Selection

Looking for how to identify a 'good' model, including number of variables, and which to keep. 


We use the UScrime dayaset to model 15 potential explanatory variables

```{r}
data("UScrime", package = "MASS")
#Using the . notation includes all variables
full_model = lm(y ~ ., data=UScrime)

```
When using the full model, the model is based on all variables combined, so we can't chuck several variables based on their p values, because they are p-values dependant on the combination of the whole model. 

## Drop 1
goes through each variable and tests what happens if it gets dropped
```{r}
drop1(full_model, test="F")
```
Variable So has the largest p-value so we test dropping that.
```{r}
model2 = update(full_model, . ~ . -So)
```

Repeat dropping different variables. We won't usually do this manually.




## Akaike information criterion
AIC = n * log (residual sum of squares / n) + 2*p

smaller is better. We are penalising adding more variables that don't reduce the residual sum of squares as much as possible. 

```{r}
#Uses AIC, not p-values
step.back.aic = step(full_model, direction = 'backward', trace = FALSE)
```

This function does this automatically, and drops variables until the 'no-drop' AIC amount is the smallest AIC, which indicates that dropping anything else will only increase AIC. 

## Backwards v forwards
sometimes we can't fit the full model, if we have more variables than observations, it simply won't work, so we start with empty model and add variables.
```{r}
empty_model = lm(y ~ 1, data=UScrime)
#we need to add another scope parameter, which defines the limits of the variables the model can include. this will usually be null/full model.
step.fwd.aic = step(empty_model, scope = list(lower=empty_model, upper = full_model), direction = 'forward', trace=FALSE)
```
Forwards and backwards models have given different models! which do we pick?!

1. can pick the simplest/less variables
2. adjusted R2 could be good if there is some different
3. if the AIC is significantly different

## Forwards selections
Start with empty model, add one var at a time
use the add1() function.
```{r}
m0 = lm(y ~ 1, data = UScrime)
add1(m0, scope = full_model, data = UScrime, test = "F", trace = FALSE)
```
There is also an option for the step function that allows both
step.both.aic()
When using both, it will make a slight difference if you start from empty or full model.

## Exhaustive Searches

Look at every single possible model, combination, outside of stepwise methods.
Uses some branching logic to eliminate checking EVERYTHING.
This package outputs all the different models for num of vars. 

```{r}
exhaustive = leaps::regsubsets(y ~ ., data=UScrime, nvmax = 15)
summary(exhaustive)$outmat
# note that size 8 drops GDP and picks up U1, stepwise can't go back and undo an addition of var
```


# L30


## in sample performance

Testing model on data used in the model itself. This doesn't help testing overfitting. Same ideas as not using split data for ML model, kinda silly, just confirms the code didn't upfucky.

Can be done by comparing partial/one var linear regression to full models.


## Out of sample performance

Test on data not used in the model. Basically just use train test split like every other method.
```{r}
#loads data
data(environmental, package = "lattice")
#adds a new lozone var that is log(ozone)
environmental = environmental |> 
  mutate(lozone = log(ozone))

n = nrow(environmental)

#num of training datapoints to create, 80% of num of rows
n_train = floor(0.8*n)
#test is the remaining 20%
n_test = n - n_train
#vector of group labels using repetition
grp_labs = rep(c("Train","Test"), times = c(n_train, n_test)) 
#assign group labels to the environmental data
environmental$grp = sample(grp_labs)
#separate the train data into a sub dataset
train_dat = environmental |> filter(grp == "Train")
#linear model for train data with temperature var
lm_simple_train = lm(lozone ~ temperature, data = train_dat)
#lm for full vars
lm_full_train = lm(lozone ~ radiation + temperature + wind, data = train_dat)
#separate test subdataset
test_dat = environmental |> filter(grp == "Test")
#prediction for simple lm
simple_pred = predict(lm_simple_train, newdata = test_dat)
#pred for full model
full_pred = predict(lm_full_train, newdata = test_dat)

#we'd like to see greater accuracy/lower errors on the full_pred than simple_pred
#Use Root mean square error.
simple_mse = mean((test_dat$lozone - simple_pred)^2)
sqrt(simple_mse)
full_mse = mean((test_dat$lozone - full_pred)^2)
sqrt(full_mse)

#Mean absolute error is less influenced by outliers than RMSE
simple_mae = mean(abs(test_dat$lozone - simple_pred))
simple_mae
full_mae = mean(abs(test_dat$lozone - full_pred))
full_mae
```

## Cross validation

Comp3308 anyone? 
k-fold cross validation. 
divide data into k subsets, for k in ksubsets, k = test, train on the rest, test on k.
average the error rates over k runs.

```{r}
set.seed(2)
n = nrow(environmental)

environmental$grp = NULL # remove the grp variable we added previously
# Create a vector of fold IDs (11 folds)
fold_id <- c(1, rep(1:10, each = 11))

# Randomly assign fold IDs to each observation in the "environmental" dataset
environmental$fold_id <- sample(fold_id, replace = FALSE)
#now each row has a column fold_id that is 1-11.

#calc model, leave 1 fold out, preds and error rates
k = 10
simple_mse = full_mse = vector(mode = "numeric", length = k)
simple_mae = full_mae = vector(mode = "numeric", length = k)
for(i in 1:k) { 
  test_set = environmental[fold_id == i,]
  training_set = environmental[fold_id != i,]
  simple_lm = lm(lozone ~ temperature, data = training_set)
  simple_pred = predict(simple_lm, test_set)
  simple_mse[i] = mean((test_set$lozone - simple_pred)^2)
  simple_mae[i] = mean(abs(test_set$lozone - simple_pred))
  full_lm = lm(lozone ~ radiation + temperature + wind, data = training_set)
  full_pred = predict(full_lm, test_set)
  full_mse[i] = mean((test_set$lozone - full_pred)^2)
  full_mae[i] = mean(abs(test_set$lozone - full_pred))
}

#aggregate errors over folds, find ave, rmse
cv_res = tibble(simple_mse, full_mse, 
                simple_mae, full_mae)
cv_sum_res = cv_res |> 
  summarise(
    across(.cols = everything(), 
           mean)
  )

sqrt(cv_sum_res[,1:2])
```

caret library - Classification And REgression training
```{r}
library(caret)
cv_full = train(
  lozone ~ radiation + temperature + wind, environmental,
  method = "lm",
  trControl = trainControl(
    method = "cv", number = 10,
    verboseIter = FALSE
  )
)
cv_full
```

# L31

## Logistic regression

Using the titanic survival stats, lets try model the relationship between sex, passenger class, and survival. There def is a relationship.

We can't really use linear model. Ideally, the Y predictor variable is normally distributed. If we're trying to predict a binary var, it doesn't work, so we'll use a logistic regression. 

If we try fit a straight line on a binary plot, it really doesn't work, and even if we let it curve off at the top and bottom, it still isn't very close. 

Modelling p values is tricky to put a straight line through, but its bounded by 0,1/
Odds = p/1-p, no longer bound by [0,1], but looks more exponential, so def no straight line. 
log(odds) range is symmetrical around 0, eg [-4,4], and is a straight line. 
We'll say Y given x follows a Bernoulli distribution, and is dependent on the observed values and coefficients. 

## Bernoulli distribution

discrete probability distribution. represents random var that has a binary outcome. 
P(X = x) = p^x * (1-p)^1-x
where X is the outcome
p is prob(X=1)
1-p is prob(X=0)
x is 0 or 1

recap: odds = P(E)/1-P(E)

## logistic regression

```{r}
library(tidyverse)
#install.packages("vcdExtra")
data("Titanicp", package = "vcdExtra")

x = Titanicp %>% mutate(survived = ifelse(survived == "survived", 1, 0))

#generalised linear model
glm1 = glm(survived ~ pclass + sex + age, family = binomial, data = x)
summary(glm1)
```
z-value in glm is similar to T-value in a lm. It assess significance of estimated coefficients, and is the number of standard deviations that the estimate is different from the H0 value. z = estimate/std error, same as T-score in lm. 
So bigger z-value is more significant. 

The final model would be written down as 
logit(p) = 3.5 - 1.3* 2ndclass -2.3* 3rdclass -2.5 * male -.03 * age

logit(p)?

= log(p/1-p)
aka log of odds ratio


## Evaluating performance for logistic regression

we need to pick a threshold to round the logit results, usually 0.5. 
We can also add the predictions to the original dataset columns, so that we can compare easily. 

p = 0.5? rounded to 0, but not ideal. 

Resubstitution error rate is basically just in sample perfomance checking.
Proportion of observations incorrectly measured. 

Out of sample performance would just be k-fold CV, same is in linear regression. 

## Confusion matrix

matrix of prediction, and counts. 
we can count the successful predictions, and separate for survivors, deaths, and misprediction rate for each. 


# L32

## Decision trees

remember COMP3308? 
"determines predicted outcome based on series of questions/conditions"
Iris dataset gives flower measurements and species, lets try classify species based on measurements. 

rpart package - recursive paritioning, to identify dependent var and precitive variables.
```{r}
library(rpart)
library(rpart.plot)
tree = rpart(Species ~ ., data = iris, method = "class") #class meaning classification tree
tree
rpart.plot(tree)
```
Alternative visualisation with partykit, partytitioning tool
```{r}
# install.packages("partykit") # A Toolkit for Recursive Partytioning
library(partykit)
plot(as.party(tree))
```



## Building a decision tree

the algo look for the most divisive/complete separations in the data. In the following, draw a line for petal length < 2.5, and then petal width > 1.75
```{r}
p1 = iris |> ggplot() +
  aes(x = Petal.Length, 
      y = Petal.Width, 
      colour = Species) + 
  geom_point(size = 4)
p1
```

there is a complexity parameter that determines if the new split sufficiently improves the predictive power or not


## Using the DT

can use the predict() function
predict(tree_name, data_frame, type="class")


## Decision tree in sample performance

As before, make a confusion matrix. 

```{r}
library(caret)
predicted_species = predict(tree, type = "class")
confusionMatrix(data = predicted_species, reference = iris$Species)
```

Remember 1 rule algorithm? eg for the titanic survivors, assume everyone dies lol, you get accuracy of 809/(809+500) = 62%
always consider a null model.

```{r}
# Step 1: Select the specified columns (survived, sex, age, pclass) from the Titanicp data frame
titanic_complete = Titanicp |> select(survived, sex, age, pclass)

# Step 2: Remove rows with missing values (NA) from the selected columns
titanic_complete = titanic_complete |> drop_na()

# Step 3: Train a model using the rpart method (recursive partitioning) for binary classification
#         The response variable is 'survived', and predictors are 'sex', 'age', and 'pclass'
#         Method used for training is "cv" (cross-validation) with 10 folds
#         This uses the train function from the caret package
trained_model = train(
  survived ~ sex + age + pclass,           # Formula specifying the model
  data = titanic_complete,                 # Training data
  method = "rpart",                        # Method for training (rpart: recursive partitioning)
  trControl = trainControl(method = "cv", number = 10)  # Cross-validation with 10 folds
)
trained_model

#outputs the complexity parameters, 0.16 optimal. feed this back into for final model
titanic_final = rpart(survived ~ sex + age + pclass, data = titanic_complete, 
                      control = rpart.control(cp = 0.016))
plot(as.party(titanic_final))
```

## DT weaknesses

can be very complex too quickly
without a complexity penalty will overfit until 100% accuracy
can only make decisions parallel to axes
overfitting in this case is just modelling noise/outliers

## Random forest

1. choose the number of decision trees, number of variables
2. random select rows WITH replacement
3. random select the vars from the data
4. build a decision tree
5. repeat the procedure
6. predict by majority rule

randomForest::randomForest() in native R defaults for 500 trees using sqrt(count_vars) per tree

```{r}
library(randomForest)
iris_rf <- randomForest(Species ~ ., iris)
iris_rf
```


# L33

## Nearest Neighbours

logistic regression doesn't work for data with perfect separation, which is when independent variables perfectly predict outcome. 
or when there is a non-linear combination of predictors

decision trees can be used for >2 classes, but they can be complicated, and only draw boundaries parallel to the axes

k-NN is non-parametric: doesn't follow a specific shape. We vote on the class based on the nearest k neighbours

euclidean distance: sqrt(difference between coords)

kNN is easy to use, implement. doesn't need any preprocessing of data

knn_k3 = knn(train = data, test = new.data, cl= y, k = 3, prob = TRUE)

## kNN +/-

+ simple to implement, 
performance improves with more samples, 
uses local information, 
easily parallelised
- only really works for quantitative data, categorical doesn't really work
can be slow, especially for new predictions
large storage requirements
high dimensions reduce accuracy
needs turning for k, small makes overfit

## Performance

in sample uses confusion matrix. good to compare for different values of k

out of sample, use folds, and check different values for k


# L34

## Clustering

in a set of scattered points, group them by proximity into clusters, which are all of the same class

##k-means clustering

1. choose a number k, will be number of clusters/classes
2. assign random[1,k] to each observation
3. for each k, compute the cluster centroid/center point
4. for each observation, reassign it to the closest centroid
repeat 3,4 until no more changes occur

kmeans_algo = kmeans(data, k = 2)

## hierarchical clustering

produces a tree or dendogram (upsidedown tree of lines splitting)

They avoid specifying how many clusters/classes to begin with
can be built bottom up/agglomerative clustering, or top down with divisive clustering

## clustering measures

between clusters

single/minimum
smallest possible distance between individual point of a cluster to another

complete/max
farthest point of cluster to farthest of another

centroidal
distance between centroids

average/mean
average distance of each observation to each obs in the other cluster

euclidean distance uses sqrt all
manhattan distance uses a grid, where euclidean will cut through corners/angle

## +/- hierarchical clustering

+
don't need to know number of clusters
can cut the hierarchy at any level to get any number, more or less
easy to interpret
deterministic = always the same, definitive

+ kmeans
can be faster
nice theoretical framework
can incorporate new data/reform clusters really easily


# L35

## Dimension reduction - principal component analysis


https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues
space expands exponentially with dimension
data is easier to work with in lower dimensionality, something math1004?

PCA produces a low dimension representation of a dataset
finds a sequence of linear combinations of variables that account for maximal variance in the data, and are mutually uncorrelated



## first principal component

make first principal component which is a combination/approximation of all the original predictors, in a linear combination

typically we scale our predictors first, like in k-nn and clustering, different scales throw stuff off a bit. We want to maximise variance, so that the PCA makes it easier to distinguish between all the different variables. 
if the variables are standardised, they have a mean 0 and stddev 1.

if they are standardised the total variance is just p where p = num of variables

## Choosing the number of principal components

proportion of variance explained per PC
Var of PCm = Vm / TV where m is the PC#

cumulative

you're looking for the point where the amount of variance takes a big drop, because from there, the variance explained by adding PCs diminishes to residuals/outliers.

```{r}
#track_pca = prcomp(data, center = TRUE, scale = TRUE)
#center and scale the data so that it is standardised, mean 0 var 1 
```






## Relative Risk
Definition: the ratio of the probability of an outcome in an exposure group divided by the likelihood in an unexposed group
eg nuclear exposure gives .8 probability of bad bad, but no exposure gives .001 probability of bad bad. RR = .8/.001 = 800

Can be calculated in a retrospective study, but you need to know the entire cohort/group that had the exposure.

## T test
Assumption on sample: independent and identically distributed (iid)
= sample_mean - expected_mean / (stddev(sample)/sqrt(n_samples))

## <- vs =

<- is for assigning values/objects to variables.
matrix(1, nrow = 2) makes a 2 row matrix
matrix(1, nrow <- 2) makes a matrix 2, but also creates a variable called nrow = 2



# 

